{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "from judges import load_judge, judge_rule_based\n",
    "from prompts import SYSTEM_MESSAGE, MUITICHOICE_TEMPLATE_PROMPT\n",
    "from conversers import load_target_model\n",
    "from utils import insert_adv_string, schedule_n_to_change_fixed, schedule_n_to_change_prob, extract_logprob, early_stopping_condition\n",
    "from additional_utils import print_and_save, judge_scores_subtask\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import json\n",
    "import copy\n",
    "from ..config import PATH_TO_DATASET\n",
    "\n",
    "os.environ[\"TOGETHERAI_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff6acf53004f709b6f399774e40647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"PATH_TO_LOG\"\n",
    "in_path = PATH_TO_DATASET\n",
    "out_path = \"PATH_TO_new_with_strings.jsonl\"\n",
    "target_token = '<'\n",
    "\n",
    "targetLM = load_target_model(\"llama3-8b\", SYSTEM_MESSAGE, target_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "\n",
    "class Args:\n",
    "    # Attack model parameters\n",
    "    attack_max_n_tokens = 500\n",
    "\n",
    "    # RS parameters\n",
    "    n_iterations = 2000\n",
    "    n_strings = 32\n",
    "    n_chars_adv = 100\n",
    "    n_chars_change_max = 0\n",
    "    n_tokens_adv = 25\n",
    "    n_tokens_change_max = 4\n",
    "    schedule_n_to_change = False\n",
    "    schedule_prob = True\n",
    "    n_restarts = 3\n",
    "\n",
    "    # Target model parameters\n",
    "    target_max_n_tokens = 250\n",
    "\n",
    "    # Judge model parameters\n",
    "    judge_model = \"gpt-4-0613\"\n",
    "    judge_max_n_tokens = 10\n",
    "    judge_temperature = 0.0\n",
    "    judge_top_p = 1.0\n",
    "    judge_max_n_calls = 12\n",
    "\n",
    "    # Logging parameters\n",
    "    index = 0\n",
    "    category = \"bomb\"\n",
    "\n",
    "    # Miscellaneous parameters\n",
    "    seed = 1\n",
    "    determinstic_jailbreak = False\n",
    "    eval_only_rs = False\n",
    "    debug = False       \n",
    "\n",
    "with open(in_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(out_path,\"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        sample = json.loads(line.strip())\n",
    "        sid = sample[\"id\"]\n",
    "        question = sample[\"question\"]\n",
    "        choices = sample[\"choices\"]\n",
    "        \n",
    "        options_str = \" \".join([f\"{chr(65 + idx)}) {choice}\" for idx, choice in enumerate(choices)])\n",
    "        letters_str = \", \".join([ chr(65 + idx) for idx, _ in enumerate(choices)])\n",
    "        args = Args()\n",
    "        strings = []\n",
    "        \n",
    "        for num_run in range(args.n_restarts):\n",
    "            substitution_set = string.digits + string.ascii_letters + string.punctuation + ' '\n",
    "            n_iterations_target_not_in_topk, n_retry_final = 25, 1\n",
    "\n",
    "            tokenizer, max_token_value = targetLM.model.tokenizer, targetLM.model.tokenizer.vocab_size\n",
    "\n",
    "            orig_msg = MUITICHOICE_TEMPLATE_PROMPT.format(\n",
    "                        question=question,\n",
    "                        choices=options_str,\n",
    "                        letters=letters_str,\n",
    "                    )\n",
    "            \n",
    "            \n",
    "            print_and_save(f'Original message: {orig_msg}', path + f\"/output_{sid}_{num_run}.txt\")\n",
    "\n",
    "            # below: different initialization for different models\n",
    "            if args.n_chars_change_max > 0:\n",
    "                adv_init = '_' * args.n_chars_adv\n",
    "            if args.n_tokens_change_max > 0:\n",
    "                adv_init = ' !' * args.n_tokens_adv\n",
    "\n",
    "            best_adv_tokens = adv_tokens = tokenizer.encode(adv_init)  #[:args.n_tokens_adv]\n",
    "\n",
    "            early_stop_rs = False\n",
    "            n_chars_change, n_tokens_change = args.n_chars_change_max, args.n_tokens_change_max\n",
    "            best_adv = adv = adv_init\n",
    "            best_msg = msg = insert_adv_string(orig_msg, adv)\n",
    "            best_logprob = -np.inf\n",
    "            best_logprobs, best_advs, logprob_dict = [], [], {}\n",
    "            judge_n_calls = 0\n",
    "\n",
    "            for it in tqdm(range(1, args.n_iterations + 1), desc=\"Processing\"):\n",
    "                # note: to avoid an extra call to get_response(), for args.determinstic_jailbreak==True, the logprob_dict from the previous iteration is used\n",
    "                judged_text = None\n",
    "                \n",
    "                if (not early_stopping_condition(best_logprobs, targetLM, logprob_dict, target_token, args.determinstic_jailbreak)):\n",
    "                    logprob = targetLM.read_log_prob(msg)\n",
    "                    \n",
    "                else:  # early stopping criterion (important for query/token efficiency)\n",
    "                    temperature = 0.0\n",
    "                    logprob = targetLM.read_log_prob(msg)\n",
    "                    \n",
    "                    output = targetLM.get_response([msg], max_n_tokens=1, temperature=temperature)[0]\n",
    "                    beginning_token = output['text']\n",
    "                    # if args.debug:\n",
    "                    #     print(final_response_text)\n",
    "                    if (beginning_token.strip() == target_token):\n",
    "                        output = targetLM.get_response([msg], max_n_tokens=args.target_max_n_tokens, temperature=temperature)[0]\n",
    "                        final_response_text = output['text']\n",
    "                        print_and_save(\"Judge %i\"%(judge_n_calls), path + f\"/output_{sid}_{num_run}.txt\")\n",
    "                        print_and_save(final_response_text, path + f\"/output_{sid}_{num_run}.txt\")\n",
    "                        \n",
    "                        if not (judged_text == final_response_text):\n",
    "                            judged_text = final_response_text\n",
    "                            llm_judge_score = judge_scores_subtask(question, [final_response_text])[0]\n",
    "                            \n",
    "                            jailbroken_judge_rule = judge_rule_based(final_response_text)\n",
    "                            judge_n_calls += 1\n",
    "                            if llm_judge_score or judge_n_calls == args.judge_max_n_calls:\n",
    "                                early_stop_rs = True\n",
    "\n",
    "                if it%50 ==0:\n",
    "                    print_and_save(f'it={it} [best] logprob={best_logprob:.3f} prob={np.exp(best_logprob):.5f}  [curr] logprob={logprob:.3f} prob={np.exp(logprob):.5f}  len_adv={len(best_adv)}/{len(best_adv_tokens)} n_change={n_chars_change}/{n_tokens_change}: {adv}', path + f\"/output_{sid}_{num_run}.txt\")\n",
    "                    \n",
    "                if logprob > best_logprob:\n",
    "                    best_logprob, best_msg, best_adv, best_adv_tokens = logprob, msg, adv, adv_tokens\n",
    "                else:\n",
    "                    adv, adv_tokens = best_adv, best_adv_tokens\n",
    "                best_logprobs.append(best_logprob)\n",
    "                best_advs.append(best_adv)\n",
    "\n",
    "                if early_stop_rs:\n",
    "                    break\n",
    "                \n",
    "                if args.n_chars_change_max > 0:\n",
    "                    if args.schedule_prob:\n",
    "                        n_chars_change = schedule_n_to_change_prob(args.n_chars_change_max, np.exp(best_logprob), targetLM)\n",
    "                    elif args.schedule_n_to_change:\n",
    "                        n_chars_change = schedule_n_to_change_fixed(args.n_chars_change_max, it)\n",
    "                    else:\n",
    "                        n_chars_change = args.n_chars_change_max\n",
    "                    substitute_pos_start = random.choice(range(len(adv)))\n",
    "                    substitution = ''.join(random.choice(substitution_set) for i in range(n_chars_change))\n",
    "                    adv = adv[:substitute_pos_start] + substitution + adv[substitute_pos_start+n_chars_change:]\n",
    "                if args.n_tokens_change_max > 0:\n",
    "                    if args.schedule_prob:\n",
    "                        n_tokens_change = schedule_n_to_change_prob(args.n_tokens_change_max, np.exp(best_logprob), targetLM)\n",
    "                    elif args.schedule_n_to_change:\n",
    "                        n_tokens_change = schedule_n_to_change_fixed(args.n_tokens_change_max, it)\n",
    "                    else:\n",
    "                        n_tokens_change = args.n_tokens_change_max\n",
    "                    substitute_pos_start = random.choice(range(len(adv_tokens)))\n",
    "                    substitution_tokens = np.random.randint(0, max_token_value, n_tokens_change).tolist()\n",
    "                    adv_tokens = adv_tokens[:substitute_pos_start] + substitution_tokens + adv_tokens[substitute_pos_start+n_tokens_change:]\n",
    "                    adv = tokenizer.decode(adv_tokens).replace('<s>', '')  # somehow, the R2D2 tokenizer inserts '<s>' at the first position\n",
    "                # apply the new adversarial suffix\n",
    "                msg = insert_adv_string(orig_msg, adv)\n",
    "            \n",
    "            \n",
    "            for it in tqdm(range(1, args.n_strings + 1), desc=\"Processing\"):\n",
    "                if not early_stop_rs:\n",
    "                    msg = copy.deepcopy(best_msg)\n",
    "                    \n",
    "                temperature = 0.0\n",
    "                logprob = targetLM.read_log_prob(msg)\n",
    "                output = targetLM.get_response([msg], max_n_tokens=1, temperature=temperature)[0]\n",
    "                beginning_token = output['text']\n",
    "\n",
    "                if (beginning_token.strip() == target_token) or (not early_stop_rs):\n",
    "                    strings.append(msg)\n",
    "                    \n",
    "                if logprob > best_logprob:\n",
    "                    best_logprob, best_msg, best_adv, best_adv_tokens = logprob, msg, adv, adv_tokens\n",
    "                else:\n",
    "                    adv, adv_tokens = best_adv, best_adv_tokens\n",
    "                best_logprobs.append(best_logprob)\n",
    "                best_advs.append(best_adv)\n",
    "\n",
    "                # try a different substitution\n",
    "                if args.n_chars_change_max > 0:\n",
    "                    if args.schedule_prob:\n",
    "                        n_chars_change = schedule_n_to_change_prob(args.n_chars_change_max, np.exp(best_logprob), targetLM)\n",
    "                    elif args.schedule_n_to_change:\n",
    "                        n_chars_change = schedule_n_to_change_fixed(args.n_chars_change_max, it)\n",
    "                    else:\n",
    "                        n_chars_change = args.n_chars_change_max\n",
    "                    substitute_pos_start = random.choice(range(len(adv)))\n",
    "                    substitution = ''.join(random.choice(substitution_set) for i in range(n_chars_change))\n",
    "                    adv = adv[:substitute_pos_start] + substitution + adv[substitute_pos_start+n_chars_change:]\n",
    "                if args.n_tokens_change_max > 0:\n",
    "                    if args.schedule_prob:\n",
    "                        n_tokens_change = schedule_n_to_change_prob(args.n_tokens_change_max, np.exp(best_logprob), targetLM)\n",
    "                    elif args.schedule_n_to_change:\n",
    "                        n_tokens_change = schedule_n_to_change_fixed(args.n_tokens_change_max, it)\n",
    "                    else:\n",
    "                        n_tokens_change = args.n_tokens_change_max\n",
    "                    substitute_pos_start = random.choice(range(len(adv_tokens)))\n",
    "                    substitution_tokens = np.random.randint(0, max_token_value, n_tokens_change).tolist()\n",
    "                    adv_tokens = adv_tokens[:substitute_pos_start] + substitution_tokens + adv_tokens[substitute_pos_start+n_tokens_change:]\n",
    "                    adv = tokenizer.decode(adv_tokens).replace('<s>', '')  # somehow, the R2D2 tokenizer inserts '<s>' at the first position\n",
    "                # apply the new adversarial suffix\n",
    "                msg = insert_adv_string(orig_msg, adv)\n",
    "\n",
    "            if not early_stop_rs:\n",
    "                for i in range(n_retry_final):\n",
    "                    # if we didn't find a jailbreak, then use temperature=1 to possibly find it within `n_retry_final` restarts\n",
    "                    final_response_text = targetLM.get_response([best_msg], max_n_tokens=args.target_max_n_tokens, temperature=1)[0]['text']\n",
    "                    if final_response_text.strip().startswith(target_token):\n",
    "                        break\n",
    "                llm_judge_score = judge_scores_subtask(question, [final_response_text])[0]\n",
    "                jailbroken_judge_llm = llm_judge_score == 10\n",
    "                jailbroken_judge_rule = judge_rule_based(final_response_text)\n",
    "\n",
    "\n",
    "            print_and_save(f'final_response_text: {final_response_text}\\n\\n', path + f\"/output_{sid}_{num_run}.txt\")\n",
    "            print_and_save(f'max_prob={np.exp(best_logprob)}, judge_llm_score= {llm_judge_score}/10, jailbroken_judge_rule={jailbroken_judge_rule}, tokens={targetLM.n_input_tokens}/{targetLM.n_output_tokens}, adv={best_adv}', path + f\"/output_{sid}_{num_run}.txt\")\n",
    "            print_and_save('\\n\\n\\n', path + f\"/output_{sid}_{num_run}.txt\")\n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        sample[\"extra_strings\"] = strings\n",
    "        fout.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
    "        clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
